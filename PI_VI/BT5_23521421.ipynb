{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yhSyhfEy4XSD"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "from gymnasium.envs.toy_text.taxi import TaxiEnv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JU8Q1qMxD6Po"
   },
   "outputs": [],
   "source": [
    "def play(env, policy, render=False):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy[state]\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        if render:\n",
    "            print(env.render())\n",
    "            time.sleep(0.5)\n",
    "            if not done:\n",
    "                display.clear_output(wait=True)\n",
    "        state = next_state\n",
    "\n",
    "    return (total_reward, steps)\n",
    "\n",
    "def play_multiple_times(env, policy, max_episodes):\n",
    "    success = 0\n",
    "    list_of_steps = []\n",
    "    for i in range(max_episodes):\n",
    "        total_reward, steps = play(env, policy)\n",
    "\n",
    "        if total_reward > 0:\n",
    "            success += 1\n",
    "            list_of_steps.append(steps)\n",
    "\n",
    "    print(f'Number of successes: {success}/{max_episodes}')\n",
    "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, max_iters=500, gamma=0.9):\n",
    "    # initialize\n",
    "    v_values = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        prev_v_values = np.copy(v_values)\n",
    "\n",
    "        # update the v-value for each state\n",
    "        for state in range(env.observation_space.n):\n",
    "            q_values = []\n",
    "\n",
    "            # compute the q-value for each action that we can perform at the state\n",
    "            for action in range(env.action_space.n):\n",
    "                q_value = 0\n",
    "                # loop through each possible outcome\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
    "\n",
    "                q_values.append(q_value)\n",
    "\n",
    "            # select the max q-values\n",
    "            best_action = np.argmax(q_values)\n",
    "            v_values[state] = q_values[best_action]\n",
    "\n",
    "        # check convergence\n",
    "        if np.all(np.isclose(v_values, prev_v_values)):\n",
    "            print(f'Converged at {i}-th iteration.')\n",
    "            break\n",
    "\n",
    "    return v_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jb0an7gaV39e"
   },
   "outputs": [],
   "source": [
    "def policy_extraction(env, v_values, gamma=0.9):\n",
    "    # initialize\n",
    "    policy = np.zeros(env.observation_space.n, dtype=np.int32)\n",
    "\n",
    "    # loop through each state in the environment\n",
    "    for state in range(env.observation_space.n):\n",
    "        q_values = []\n",
    "        # loop through each action\n",
    "        for action in range(env.action_space.n):\n",
    "            q_value = 0\n",
    "            # loop each possible outcome\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                q_value += prob * (reward + gamma * v_values[next_state])\n",
    "\n",
    "            q_values.append(q_value)\n",
    "\n",
    "        # select the best action\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[state] = best_action\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YOQ7Hs4DqX2T"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def policy_evaluation(policy, env, gamma=0.9, theta=1e-8):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = 0\n",
    "            a = policy[s]\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                v += prob * (reward + gamma * V[next_state])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def policy_improvement(env, V, gamma=0.9):\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    for s in range(env.observation_space.n):\n",
    "        q_values = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                q_values[a] += prob * (reward + gamma * V[next_state])\n",
    "        policy[s] = np.argmax(q_values)\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(env, gamma=0.9, theta=1e-8):\n",
    "    policy = np.random.choice(env.action_space.n, size=(env.observation_space.n,))\n",
    "    while True:\n",
    "        V = policy_evaluation(policy, env, gamma, theta)\n",
    "        new_policy = policy_improvement(env, V, gamma)\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Value Iteration on FrozenLake-v1\n",
      "Converged at 79-th iteration.\n",
      "Number of successes: 787/1000\n",
      "Average number of steps: 45.00381194409149\n",
      "Runtime: 0.22332525253295898\n",
      "--------------------------------------------------\n",
      "Running Value Iteration on FrozenLake8x8-v1\n",
      "Converged at 117-th iteration.\n",
      "Number of successes: 731/1000\n",
      "Average number of steps: 74.12311901504788\n",
      "Runtime: 0.4404423236846924\n",
      "--------------------------------------------------\n",
      "Running Value Iteration on Taxi-v3\n",
      "Converged at 116-th iteration.\n",
      "Number of successes: 1000/1000\n",
      "Average number of steps: 13.109\n",
      "Runtime: 0.3302128314971924\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env_names = ['FrozenLake-v1', 'FrozenLake8x8-v1', 'Taxi-v3']\n",
    "max_episodes = 1000\n",
    "\n",
    "for env_name in env_names:\n",
    "    start = time.time()\n",
    "    print(f\"Running Value Iteration on {env_name}\")\n",
    "\n",
    "    if 'FrozenLake' in env_name:\n",
    "        env_train = gym.make(env_name, is_slippery=True).unwrapped\n",
    "    else:\n",
    "        env_train = gym.make(env_name).unwrapped\n",
    "\n",
    "    # Run value iteration\n",
    "    v_values = value_iteration(env_train, gamma=0.9)\n",
    "\n",
    "    # Extract policy from value function\n",
    "    policy = policy_extraction(env_train, v_values, gamma=0.9)\n",
    "\n",
    "    # Evaluate with wrapper (keep TimeLimit etc.)\n",
    "    env_eval = gym.make(env_name)\n",
    "    play_multiple_times(env_eval, policy, max_episodes)\n",
    "    end = time.time()\n",
    "    runtime = end - start\n",
    "    print(f\"Runtime: {runtime}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Policy Iteration on FrozenLake-v1\n",
      "Optimal Policy:\n",
      "[[0 3 0 3]\n",
      " [0 0 0 0]\n",
      " [3 1 0 0]\n",
      " [0 2 1 0]]\n",
      "Number of successes: 786/1000\n",
      "Average number of steps: 43.157760814249365\n",
      "Runtime: 0.17296195030212402\n",
      "--------------------------------------------------\n",
      "Running Policy Iteration on FrozenLake8x8-v1\n",
      "Optimal Policy:\n",
      "[[3 2 2 2 2 2 2 2]\n",
      " [3 3 3 3 2 2 2 1]\n",
      " [3 3 0 0 2 3 2 1]\n",
      " [3 3 3 1 0 0 2 1]\n",
      " [3 3 0 0 2 1 3 2]\n",
      " [0 0 0 1 3 0 0 2]\n",
      " [0 0 1 0 0 0 0 2]\n",
      " [0 1 0 0 1 1 1 0]]\n",
      "Number of successes: 736/1000\n",
      "Average number of steps: 74.64673913043478\n",
      "Runtime: 0.28209447860717773\n",
      "--------------------------------------------------\n",
      "Running Policy Iteration on Taxi-v3\n",
      "Optimal Policy:\n",
      "[4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
      " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 2 2 2\n",
      " 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
      " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 2 2 2 2 3 3 3 3 2 2 2 2 3 2 3\n",
      " 2 3 3 3 3 2 2 2 2 3 3 3 3 0 0 0 0 3 2 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
      " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 2 2 2 2 1 1 1 1 2\n",
      " 2 2 2 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 1\n",
      " 1 1 0 0 0 0 1 2 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 2 2 2 2 1 1 1 1 2 2 2 2 1 2 1 2 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 1 1 1 4 4 4 4 1 2 1 5 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3]\n",
      "Number of successes: 1000/1000\n",
      "Average number of steps: 13.046\n",
      "Runtime: 0.7607612609863281\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env_names = ['FrozenLake-v1', 'FrozenLake8x8-v1', 'Taxi-v3']\n",
    "\n",
    "for env_name in env_names:\n",
    "    start = time.time()\n",
    "    print(f\"Running Policy Iteration on {env_name}\")\n",
    "    \n",
    "    if 'FrozenLake' in env_name:\n",
    "        env = gym.make(env_name, is_slippery=True).unwrapped\n",
    "    else:\n",
    "        env = gym.make(env_name).unwrapped\n",
    "\n",
    "    policy, V = policy_iteration(env)\n",
    "    \n",
    "    print(\"Optimal Policy:\")\n",
    "    if 'FrozenLake' in env_name:\n",
    "        size = int(np.sqrt(env.observation_space.n))\n",
    "        print(policy.reshape((size, size)))\n",
    "        play_multiple_times(env, policy, 1000)\n",
    "    else:\n",
    "        print(policy)\n",
    "        play_multiple_times(env, policy, 1000)\n",
    "    end = time.time()\n",
    "    runtime = end - start\n",
    "    print(f\"Runtime: {runtime}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy iteration nhanh hơn ở các môi trường nhỏ hoặc vừa, do đánh giá chính sách không quá tốn kém.\n",
    "\n",
    "value Iteration nhannh hơn khi số lượng trạng thái lớn( policy evaluation của policy iterationn tốn kém khi số lượng trạng thái lớn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FrozenLake-v1 và FrozenLake8x8-v1\n",
    "tỉ lệ thành công gần như giống nhau (786/787 và 736/731).\n",
    "số bước trung bình tương đương (khác biệt rất nhỏ).\n",
    "Policy Iteration chạy nhanh hơn.\n",
    "\n",
    "Nhận xét:\n",
    "\n",
    "Với môi trường nhỏ và không quá nhiều trạng thái, Policy Iteration tận dụng được ưu điểm là ít vòng lặp hơn.\n",
    "Đặc biệt với FrozenLake8x8, tốc độ của Policy Iteration nhanh gần gấp đôi Value Iteration.\n",
    "\n",
    "Taxi-v3\n",
    "Cả hai đều đạt 100% thành công và số bước tương đương.\n",
    "Nhưng Policy Iteration tốn hơn gấp đôi thời gian so với Value Iteration.\n",
    "\n",
    "Nhận xét:\n",
    "Với môi trường lớn hơn (Taxi có 500 states), chi phí tính toán trong mỗi vòng Policy Evaluation tăng lên.\n",
    "Value Iteration cho kết quả tốt hơn về thời gian do mỗi vòng lặp rẻ hơn, dù cần nhiều vòng hơn."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
